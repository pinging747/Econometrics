{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The level of statistical significance is often expressed as a p-value between 0 and 1. \n",
    "#The smaller the p-value, the stronger the evidence that you should reject the null hypothesis.\n",
    "\n",
    "# p-value less than 0.05 (typically ≤ 0.05)\n",
    "        #statistically significant\n",
    "        #evidence against the null hypothesis\n",
    "        #less than a 5% probability the null is correct\n",
    "        #reject the null hypothesis\n",
    "        #accept the alternative hypothesis\n",
    "        \n",
    " # p-value higher than 0.05 (> 0.05)     \n",
    "    # not statistically significant \n",
    "    # strong evidence for the null hypothesis\n",
    "    #retain/ (fail to reject it) the null hypothesis\n",
    "    #reject the alternative hypothesis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example in the book\n",
    "# We see that with a value of only 0.444, the t-ratio\n",
    "# is very small, which indicates that the corresponding null hypothesis H0 : a = 0 is likely not to be\n",
    "# rejected. Turning to the slope estimate for 'ret_future', the t-ratio is high with 146.543 suggesting that\n",
    "# H0 : b = 0 is to be rejected against the alternative hypothesis of H1 : b 6= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The p-values presented\n",
    "# in the fourth column, 'P>|t|', confirm our expectations: the p-value for the constant is considerably\n",
    "# larger than 0.1, meaning that the corresponding t-statistic is not even significant at a 10% level; in\n",
    "# comparison, the p-value for the slope coefficient is zero to, at least, three decimal places. Thus, the\n",
    "# null hypothesis for the slope coefficient is rejected at the 1% level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Futures is set\n",
    "#as explanatory variable and Spot is the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'C:\\\\Users\\\\Rameez PC\\\\Documents\\\\GitHub\\\\Econometrics\\\\hypothesis testing a\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= pd.read_excel(path + 'SandPhedge.xls', index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spot</th>\n",
       "      <th>Futures</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-02-01</th>\n",
       "      <td>1106.73</td>\n",
       "      <td>1106.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-03-01</th>\n",
       "      <td>1147.39</td>\n",
       "      <td>1149.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-04-01</th>\n",
       "      <td>1076.92</td>\n",
       "      <td>1077.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-05-01</th>\n",
       "      <td>1067.14</td>\n",
       "      <td>1067.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-06-01</th>\n",
       "      <td>989.82</td>\n",
       "      <td>990.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Spot  Futures\n",
       "Date                        \n",
       "2002-02-01  1106.73   1106.9\n",
       "2002-03-01  1147.39   1149.2\n",
       "2002-04-01  1076.92   1077.2\n",
       "2002-05-01  1067.14   1067.5\n",
       "2002-06-01   989.82    990.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can follow the steps described above and specify\n",
    "# 'ret_spot' as the dependent variable and ’ret_future’ as the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'Spot ~ Futures'\n",
    "# ß is Futures. Null Hypothesis is ß=1. \n",
    "hypotheses = 'Futures = 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=array([[5.42447367]]), p=0.02136355027890708, df_denom=133, df_num=1>\n"
     ]
    }
   ],
   "source": [
    "# we repeat the regression procedure\n",
    "#again.\n",
    "results = smf.ols(formula, file).fit()\n",
    "# We can, of course, calculate the test statistics for this hypothesis test by hand; however, it is\n",
    "# easier if we let Python do this work. For this we use the Statsmodels function 'f_test'.\n",
    "f_test = results.f_test(hypotheses)\n",
    "print(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we find the test statistics: 'F=array([[\n",
    "# 5.42447367]])', which states that the value of the F-test is around 5.42. The corresponding p-value\n",
    "# is 0.02, stated in the next position. As it is smaller than 0.10, we clearly can reject the null hypothesis\n",
    "# that the coefficient estimate is equal to 1. The last two numbers present the total number of\n",
    "# observations and the degrees of freedom for this test respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate a new data series of continuously compounded returns, we define a new function\n",
    "# in Python to achieve this calculation (see In [4]). Specifically, we create a user-defined function\n",
    "# called LogDiff where the input parameter is a Pandas column.12 To calculate the log difference,\n",
    "# we first obtain a column which lags one period. This can be done by typing the command x.shift(1).\n",
    "# Then, we take the log transformation of the difference between the original and lagged series. The\n",
    "# function log comes from the NumPy library, so we can use this function by entering the command\n",
    "# np.log(x/x.shift(1)). Next, we want to output the result expressed as a percentage. Therefore, the\n",
    "# column is scaled by multiplying it by 100. It is worth noting that the first price observation will\n",
    "# be lost when return series is computed. However, the first data point will still be displayed in the\n",
    "# DataFrame as nan since Python keeps the length of the DataFrame intact. To avoid this pitfall, we\n",
    "# employ the Pandas dropna() function remove it. Finally, we return the newly-calculated column.\n",
    "# We have to call the function itself outside the function content. This can be done by typing the\n",
    "# command LogDiff(data[’Spot’]). Meanwhile, we also rebuild the data DataFrame which previously\n",
    "# stored the prices data. A newly created DataFrame can be defined by using the Pandas DataFrame\n",
    "# function. In the bracket of the DataFrame function, we specify two column names: ret_spot and\n",
    "# ret_future respectively. To fill out their values, the LogDiff function is called to compute the new\n",
    "# series. Once finished, we can repeat the process as stated above to print the finalised DataFrame. As\n",
    "#can be seen in Out [4], the newly created DataFrame starts from October 1997 instead of September\n",
    "# 1997 and the column names have changed from Spot and Futuresto ret_spot and ret_future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogDiff(x):\n",
    "    x_diff = 100*np.log(x/x.shift(1))\n",
    "    x_diff = x_diff.dropna()\n",
    "    return x_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'ret_spot' : LogDiff(file['Spot']),\n",
    "'ret_future':LogDiff(file['Futures'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'ret_spot ~ ret_future'\n",
    "hypotheses = 'ret_future = 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=array([[1.54521196]]), p=0.21604712494294365, df_denom=132, df_num=1>\n"
     ]
    }
   ],
   "source": [
    "results = smf.ols(formula, data).fit()\n",
    "f_test = results.f_test(hypotheses)\n",
    "print(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With an F-statistic of 0.02 and a corresponding p-value of nearly 0.8, we find that the null hypothesis\n",
    "# is failed to be rejected at the 1% significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 32-bit",
   "language": "python",
   "name": "python37232bit6ed36a65d3434a11802d64fc314a2148"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
